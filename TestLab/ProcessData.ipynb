{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52882b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrer los archivos csv, realizar el procesamiento de los datos y guardar en una nueva carpeta los archivos csv resultante\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from nltk.stem import SnowballStemmer, RegexpStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import regexp_tokenize as rxtoken\n",
    "import os, re, random, nltk, pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de96f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(path):\n",
    "    for dirname, subdir, listfile in os.walk(path): # Recorro el directorio\n",
    "            for file in listfile: # Recorro los archivos en el directorio\n",
    "                if file.endswith('.csv'): # Solo los archivos .csv\n",
    "                    processFiles(file) # Paso cada archivo a la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c194cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    try:\n",
    "        #token = word_tokenize(text) # Separa el texto en tokens\n",
    "        token = regexp_tokenize(text,'[^0-9]') # Extrae los token del texto sin numeros\n",
    "        token = ''.join(str(k) for k in token) # Une los items de la lista\n",
    "        return token.split() # Devuelve tokens del texto\n",
    "    except TypeError as te:\n",
    "        raise Exception(\"Ocurrió un error: {} \\n Contenido: {}\".format(te, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d85088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    stemmer = SnowballStemmer('english') # Clase Stemming\n",
    "    stem_text = []\n",
    "    for word in text: # Recorro cada palabra en la lista y extraigo su raiz\n",
    "        stem_text.append(stemmer.stem(word))\n",
    "    return stem_text # Devuelvo la raiz de cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ada2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    stop_words_list = stopwords.words('english')\n",
    "    new_stop_words = ['hehe','milw','milwauke','kewl','fer','ugh','kewl','regrowin','btw','lol','aww','sez','hmmmmm','idk']\n",
    "    stop_words_list.extend(new_stop_words)\n",
    "    token = tokenization(text)\n",
    "    clean_words = [word for word in token if len(word)>3 and word not in stop_words_list]\n",
    "    lemma_words = lemmatization(clean_words)\n",
    "    proces_text = [text for text in lemma_words if text]\n",
    "    proces_text = ' '.join(str(t) for t in proces_text if t)\n",
    "    return proces_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb84183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFiles(file):\n",
    "    df_file = pd.read_csv(f\"Dataset/{file}\") # Guardo el archivo csv en un objeto\n",
    "    #df_file['Text'].replace('', np.nan, inplace=True) # Remplazo los strings vacion por NaN\n",
    "    #df_file = df_file.dropna() # Elimino las filas con valores NaN\n",
    "    #df_file['Text'] = df_file['Text'].apply(processText) # Aplico la función al texto del Dataframe\n",
    "    #df_file['Text'].replace('', np.nan, inplace=True) # Remplazo los strings vacion por NaN\n",
    "    #df_file = df_file.dropna() # Elimino las filas con valores NaN\n",
    "    df_file.to_csv('Dataset/Chats.csv', index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d12deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/PARA/Documents/Scripts/Python/Dataset\"\n",
    "get_csv_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d78d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
